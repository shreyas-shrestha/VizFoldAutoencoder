{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyas-shrestha/VizFoldAutoencoder/blob/main/vizfoldfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88fd1e34",
        "outputId": "208a7891-6c3d-4920-e34f-7bb4cca9f125"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if the protein data directory exists\n",
        "proteins_dir = \"Proteins_layer47\"\n",
        "if os.path.exists(proteins_dir):\n",
        "    npy_files = list(Path(proteins_dir).glob(\"*.npy\"))\n",
        "    print(f\"Found {len(npy_files)} .npy files in '{proteins_dir}' directory:\")\n",
        "    for file_path in npy_files:\n",
        "        print(f\"  - {file_path.name}\")\n",
        "else:\n",
        "    print(f\"Warning: Directory '{proteins_dir}' not found!\")\n",
        "    \n",
        "print(\"Ready to load protein data for autoencoder training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is to define visualization functions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Visualization libraries loaded successfully!\")\n",
        "\n",
        "def visualize_kfold_splits(data_size, n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Visualize how K-fold cross-validation splits the data\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import KFold\n",
        "    \n",
        "    fig, axes = plt.subplots(n_splits, 1, figsize=(15, 2*n_splits))\n",
        "    if n_splits == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    # Create dummy data indices\n",
        "    indices = np.arange(data_size)\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    colors = ['lightblue', 'lightcoral']\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
        "        ax = axes[fold]\n",
        "        \n",
        "        # Create binary masks for visualization\n",
        "        all_indices = np.arange(data_size)\n",
        "        train_mask = np.isin(all_indices, train_idx)\n",
        "        val_mask = np.isin(all_indices, val_idx)\n",
        "        \n",
        "        # Plot each sample as a small rectangle\n",
        "        for i in range(data_size):\n",
        "            color = colors[0] if train_mask[i] else colors[1]\n",
        "            ax.barh(0, 1, left=i, height=0.5, color=color, alpha=0.7, edgecolor='none')\n",
        "        \n",
        "        ax.set_xlim(0, data_size)\n",
        "        ax.set_ylim(-0.5, 0.5)\n",
        "        ax.set_xlabel('Data Sample Index')\n",
        "        ax.set_title(f'Fold {fold+1}: Train/Validation Split')\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "        # Add statistics\n",
        "        ax.text(data_size*0.02, 0.2, f'Train: {len(train_idx)} samples ({len(train_idx)/data_size*100:.1f}%)', \n",
        "                fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[0], alpha=0.5))\n",
        "        ax.text(data_size*0.02, -0.2, f'Val: {len(val_idx)} samples ({len(val_idx)/data_size*100:.1f}%)', \n",
        "                fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[1], alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'{n_splits}-Fold Cross-Validation Data Splits', y=1.02, fontsize=14, fontweight='bold')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nK-Fold Cross-Validation Summary:\")\n",
        "    print(f\"Total samples: {data_size}\")\n",
        "    print(f\"Number of folds: {n_splits}\")\n",
        "    print(f\"Training samples per fold: ~{data_size*(n_splits-1)//n_splits}\")\n",
        "    print(f\"Validation samples per fold: ~{data_size//n_splits}\")\n",
        "    print(f\"Training/Validation ratio: {(n_splits-1)}:1\")\n",
        "\n",
        "def analyze_data_distribution(data_matrix, protein_names):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the distribution of protein data\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Overall data distribution\n",
        "    axes[0,0].hist(data_matrix.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0,0].set_title('Distribution of All Protein Data Values')\n",
        "    axes[0,0].set_xlabel('Value')\n",
        "    axes[0,0].set_ylabel('Frequency')\n",
        "    axes[0,0].axvline(data_matrix.mean(), color='red', linestyle='--', label=f'Mean: {data_matrix.mean():.3f}')\n",
        "    axes[0,0].legend()\n",
        "    \n",
        "    # 2. Sample-wise statistics\n",
        "    sample_means = np.mean(data_matrix, axis=1)\n",
        "    sample_stds = np.std(data_matrix, axis=1)\n",
        "    \n",
        "    axes[0,1].scatter(sample_means, sample_stds, alpha=0.6, color='coral')\n",
        "    axes[0,1].set_title('Sample-wise Mean vs Standard Deviation')\n",
        "    axes[0,1].set_xlabel('Sample Mean')\n",
        "    axes[0,1].set_ylabel('Sample Std Dev')\n",
        "    \n",
        "    # 3. Feature-wise statistics (sample a subset due to high dimensionality)\n",
        "    feature_subset = np.random.choice(data_matrix.shape[1], min(1000, data_matrix.shape[1]), replace=False)\n",
        "    feature_means = np.mean(data_matrix[:, feature_subset], axis=0)\n",
        "    feature_stds = np.std(data_matrix[:, feature_subset], axis=0)\n",
        "    \n",
        "    axes[1,0].scatter(feature_means, feature_stds, alpha=0.6, color='lightgreen')\n",
        "    axes[1,0].set_title(f'Feature-wise Statistics (Random {len(feature_subset)} features)')\n",
        "    axes[1,0].set_xlabel('Feature Mean')\n",
        "    axes[1,0].set_ylabel('Feature Std Dev')\n",
        "    \n",
        "    # 4. Protein count by type (if we can extract protein types)\n",
        "    protein_types = [name.split('_')[0] for name in protein_names[:min(100, len(protein_names))]]\n",
        "    type_counts = pd.Series(protein_types).value_counts()\n",
        "    \n",
        "    axes[1,1].bar(range(len(type_counts)), type_counts.values, color='gold', alpha=0.7)\n",
        "    axes[1,1].set_title('Protein Type Distribution (First 100 samples)')\n",
        "    axes[1,1].set_xlabel('Protein Type')\n",
        "    axes[1,1].set_ylabel('Count')\n",
        "    axes[1,1].set_xticks(range(len(type_counts)))\n",
        "    axes[1,1].set_xticklabels(type_counts.index, rotation=45, ha='right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nData Distribution Summary:\")\n",
        "    print(f\"Shape: {data_matrix.shape}\")\n",
        "    print(f\"Mean: {data_matrix.mean():.6f}\")\n",
        "    print(f\"Std: {data_matrix.std():.6f}\")\n",
        "    print(f\"Min: {data_matrix.min():.6f}\")\n",
        "    print(f\"Max: {data_matrix.max():.6f}\")\n",
        "    print(f\"Unique proteins: {len(set([name.split('_')[0] for name in protein_names]))}\")\n",
        "\n",
        "print(\"Visualization functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JooqLGu7_xvR",
        "outputId": "fb5800d9-7b7a-4a58-f535-fb24d8ccc8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 protein files\n",
            "Loaded data shape: (512, 490000)\n",
            "Total vectors: 512\n",
            "Vector size: 490000 (padded to 700×700 = 49,000)\n",
            "Original L dimension: 280\n",
            "\n",
            "Data ready for autoencoder:\n",
            "Shape: (512, 490000)\n",
            "Data type: float32\n",
            "Value range: [-11.610, 14.519]\n",
            "Original L dimension: 280\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Data Loading with Fixed Padding Logic\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_protein_data(data_directory=\"Proteins_layer47\", normalize=True):\n",
        "    \"\"\"\n",
        "    Load and preprocess protein data for autoencoder training.\n",
        "    Uses zero-padding to create fixed 50-dimensional vectors.\n",
        "\n",
        "    Args:\n",
        "        data_directory (str): Path to folder containing protein .npy files\n",
        "        normalize (bool): Whether to standardize the data\n",
        "\n",
        "    Returns:\n",
        "        tuple: (data_matrix, protein_names, original_L)\n",
        "            - data_matrix: numpy array of shape (n_proteins * 128, 2500)\n",
        "            - protein_names: list of protein names (repeated 128 times)\n",
        "            - original_L: int, original L dimension before padding\n",
        "    \"\"\"\n",
        "\n",
        "    data_path = Path(data_directory)\n",
        "    if not data_path.exists():\n",
        "        raise FileNotFoundError(f\"Directory {data_directory} not found\")\n",
        "\n",
        "    npy_files = list(data_path.glob(\"*.npy\"))\n",
        "    if not npy_files:\n",
        "        raise ValueError(f\"No .npy files found in {data_directory}\")\n",
        "\n",
        "    print(f\"Found {len(npy_files)} protein files\")\n",
        "\n",
        "    # Load protein data and extract vectors for each of the 128 channels\n",
        "    all_vectors = []\n",
        "    all_protein_names = []\n",
        "    original_L = None\n",
        "    target_dim = 50  # Target dimension for padding\n",
        "\n",
        "    for file_path in npy_files:\n",
        "        try:\n",
        "            # Load the protein data (L×L×128)\n",
        "            data = np.load(file_path)\n",
        "\n",
        "            if original_L is None:\n",
        "                original_L = data.shape[0]  # Store original L dimension\n",
        "\n",
        "            # Extract each of the 128 feature vectors (L×L each)\n",
        "            L = data.shape[0]\n",
        "            for channel in range(data.shape[2]):\n",
        "                # Get L×L matrix for this channel\n",
        "                channel_matrix = data[:, :, channel]  # Shape: (L, L)\n",
        "\n",
        "                # Pad to exactly 50×50 = 2,500 elements\n",
        "                if L <= target_dim:\n",
        "                    # Zero-pad to 50×50\n",
        "                    padded_matrix = np.zeros((target_dim, target_dim), dtype=channel_matrix.dtype)\n",
        "                    padded_matrix[:L, :L] = channel_matrix\n",
        "                else:\n",
        "                    # If L > 50, truncate to 50×50 (though problem states padding for L≤50)\n",
        "                    padded_matrix = channel_matrix[:target_dim, :target_dim]\n",
        "                \n",
        "                # Flatten to get 2,500 elements\n",
        "                vector = padded_matrix.flatten()\n",
        "\n",
        "                all_vectors.append(vector)\n",
        "                all_protein_names.append(f\"{file_path.stem}_ch{channel:03d}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_vectors:\n",
        "        raise ValueError(\"No protein data was successfully loaded\")\n",
        "\n",
        "    # Convert to numpy array\n",
        "    data_matrix = np.array(all_vectors)\n",
        "\n",
        "    # Normalize the data if requested\n",
        "    if normalize:\n",
        "        scaler = StandardScaler()\n",
        "        data_matrix = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    print(f\"Loaded data shape: {data_matrix.shape}\")\n",
        "    print(f\"Total vectors: {len(all_vectors)}\")\n",
        "    print(f\"Vector size: {data_matrix.shape[1]} (50×50 padded matrices)\")\n",
        "    print(f\"Original L dimension: {original_L}\")\n",
        "    \n",
        "    # Check for any problematic values\n",
        "    if np.isnan(data_matrix).any():\n",
        "        print(\"Warning: NaN values detected in data matrix!\")\n",
        "        data_matrix = np.nan_to_num(data_matrix, nan=0.0)\n",
        "    \n",
        "    if np.isinf(data_matrix).any():\n",
        "        print(\"Warning: Infinite values detected in data matrix!\")\n",
        "        data_matrix = np.nan_to_num(data_matrix, posinf=5.0, neginf=-5.0)\n",
        "    \n",
        "    print(f\"Data value range after preprocessing: [{data_matrix.min():.3f}, {data_matrix.max():.3f}]\")\n",
        "\n",
        "    return data_matrix, all_protein_names, original_L\n",
        "\n",
        "print(\"Data loading function with fixed padding ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Autoencoder Model and Training Functions\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SimpleAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=2500, proj_dim=1024, hidden_dim=128, latent_dim=8):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder: input_dim → 1024 → 128 → 8\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(proj_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Decoder: 8 → 128 → 1024 → input_dim\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(proj_dim, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def train_simple(model, train_loader, val_loader, lr, wd, device, return_history=False):\n",
        "    \"\"\"Train for 50 epochs with SGD and MSE loss\"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Track training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(50):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            data = batch[0].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                data = batch[0].to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, data).item()\n",
        "        \n",
        "        # Store losses\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch {epoch+1}/50, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "    \n",
        "    if return_history:\n",
        "        return avg_val_loss, train_losses, val_losses\n",
        "    return avg_val_loss\n",
        "\n",
        "def plot_training_curves(fold_histories, hyperparams):\n",
        "    \"\"\"\n",
        "    Plot training curves for all folds and hyperparameter combinations\n",
        "    \"\"\"\n",
        "    n_params = len(hyperparams)\n",
        "    fig, axes = plt.subplots(2, (n_params + 1) // 2, figsize=(15, 8))\n",
        "    if n_params == 1:\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, 5))  # 5 colors for 5 folds\n",
        "    \n",
        "    for param_idx, (params, histories) in enumerate(zip(hyperparams, fold_histories)):\n",
        "        ax = axes[param_idx]\n",
        "        \n",
        "        # Plot each fold\n",
        "        for fold_idx, (train_losses, val_losses) in enumerate(histories):\n",
        "            epochs = range(1, len(train_losses) + 1)\n",
        "            ax.plot(epochs, train_losses, '--', color=colors[fold_idx], alpha=0.7, \n",
        "                   label=f'Fold {fold_idx+1} Train' if param_idx == 0 else \"\")\n",
        "            ax.plot(epochs, val_losses, '-', color=colors[fold_idx], alpha=0.9,\n",
        "                   label=f'Fold {fold_idx+1} Val' if param_idx == 0 else \"\")\n",
        "        \n",
        "        ax.set_title(f'lr={params[\"lr\"]}, wd={params[\"wd\"]:.0e}')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        if param_idx == 0:\n",
        "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(n_params, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Training Curves for All Hyperparameter Combinations', y=1.02, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "def plot_hyperparameter_comparison(results_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive hyperparameter comparison plots\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Heatmap of mean scores\n",
        "    pivot_table = results_df.pivot(index='lr', columns='wd', values='mean_score')\n",
        "    sns.heatmap(pivot_table, annot=True, fmt='.6f', cmap='viridis_r', ax=axes[0,0])\n",
        "    axes[0,0].set_title('Mean CV Score Heatmap')\n",
        "    \n",
        "    # 2. Bar plot of all combinations\n",
        "    param_labels = [f\"lr={row['lr']:.0e}\\nwd={row['wd']:.0e}\" for _, row in results_df.iterrows()]\n",
        "    axes[0,1].bar(range(len(results_df)), results_df['mean_score'], \n",
        "                  yerr=results_df['std_score'], capsize=5, alpha=0.7, color='skyblue')\n",
        "    axes[0,1].set_xlabel('Hyperparameter Combination')\n",
        "    axes[0,1].set_ylabel('Mean CV Score')\n",
        "    axes[0,1].set_title('Hyperparameter Comparison with Error Bars')\n",
        "    axes[0,1].set_xticks(range(len(results_df)))\n",
        "    axes[0,1].set_xticklabels(param_labels, rotation=45, ha='right', fontsize=8)\n",
        "    \n",
        "    # 3. Learning rate comparison\n",
        "    lr_grouped = results_df.groupby('lr').agg({'mean_score': ['mean', 'std']}).reset_index()\n",
        "    lr_grouped.columns = ['lr', 'mean', 'std']\n",
        "    axes[1,0].bar(range(len(lr_grouped)), lr_grouped['mean'], \n",
        "                  yerr=lr_grouped['std'], capsize=5, alpha=0.7, color='lightcoral')\n",
        "    axes[1,0].set_xlabel('Learning Rate')\n",
        "    axes[1,0].set_ylabel('Mean CV Score')\n",
        "    axes[1,0].set_title('Learning Rate Effect')\n",
        "    axes[1,0].set_xticks(range(len(lr_grouped)))\n",
        "    axes[1,0].set_xticklabels([f'{lr:.0e}' for lr in lr_grouped['lr']])\n",
        "    \n",
        "    # 4. Weight decay comparison\n",
        "    wd_grouped = results_df.groupby('wd').agg({'mean_score': ['mean', 'std']}).reset_index()\n",
        "    wd_grouped.columns = ['wd', 'mean', 'std']\n",
        "    axes[1,1].bar(range(len(wd_grouped)), wd_grouped['mean'], \n",
        "                  yerr=wd_grouped['std'], capsize=5, alpha=0.7, color='lightgreen')\n",
        "    axes[1,1].set_xlabel('Weight Decay')\n",
        "    axes[1,1].set_ylabel('Mean CV Score')\n",
        "    axes[1,1].set_title('Weight Decay Effect')\n",
        "    axes[1,1].set_xticks(range(len(wd_grouped)))\n",
        "    axes[1,1].set_xticklabels([f'{wd:.0e}' for wd in wd_grouped['wd']])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_model_architecture(input_dim=2500):\n",
        "    \"\"\"\n",
        "    Create a visual representation of the autoencoder architecture\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
        "    \n",
        "    # Define layer dimensions and positions\n",
        "    layers = [\n",
        "        {'name': f'Input\\n{input_dim:,}', 'size': input_dim, 'pos': 0, 'color': 'lightblue'},\n",
        "        {'name': 'Projection\\n1,024', 'size': 1024, 'pos': 2, 'color': 'lightgreen'},\n",
        "        {'name': 'Hidden\\n128', 'size': 128, 'pos': 4, 'color': 'orange'},\n",
        "        {'name': 'Latent\\n8', 'size': 8, 'pos': 6, 'color': 'red'},\n",
        "        {'name': 'Hidden\\n128', 'size': 128, 'pos': 8, 'color': 'orange'},\n",
        "        {'name': 'Projection\\n1,024', 'size': 1024, 'pos': 10, 'color': 'lightgreen'},\n",
        "        {'name': f'Output\\n{input_dim:,}', 'size': input_dim, 'pos': 12, 'color': 'lightblue'}\n",
        "    ]\n",
        "    \n",
        "    max_size = max(layer['size'] for layer in layers)\n",
        "    \n",
        "    # Draw layers\n",
        "    for i, layer in enumerate(layers):\n",
        "        # Calculate height proportional to layer size (with minimum height)\n",
        "        height = max(0.5, (layer['size'] / max_size) * 4)\n",
        "        \n",
        "        # Draw rectangle for layer\n",
        "        rect = Rectangle((layer['pos'], -height/2), 1.5, height, \n",
        "                        facecolor=layer['color'], edgecolor='black', alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        \n",
        "        # Add layer label\n",
        "        ax.text(layer['pos'] + 0.75, height/2 + 0.3, layer['name'], \n",
        "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "        \n",
        "        # Draw arrows between layers\n",
        "        if i < len(layers) - 1:\n",
        "            ax.arrow(layer['pos'] + 1.5, 0, 0.4, 0, \n",
        "                    head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
        "    \n",
        "    # Add encoder/decoder labels\n",
        "    ax.text(3, -3, 'ENCODER', ha='center', va='center', fontsize=14, \n",
        "           fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgray'))\n",
        "    ax.text(9, -3, 'DECODER', ha='center', va='center', fontsize=14, \n",
        "           fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgray'))\n",
        "    \n",
        "    # Add compression ratio annotation\n",
        "    compression_ratio = input_dim / 8\n",
        "    ax.text(6, 3, f'Compression Ratio:\\n{input_dim:,} → 8\\n({compression_ratio:,.0f}:1)', \n",
        "           ha='center', va='center', fontsize=12, \n",
        "           bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='yellow', alpha=0.7))\n",
        "    \n",
        "    ax.set_xlim(-1, 14)\n",
        "    ax.set_ylim(-4, 4)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Autoencoder Architecture', fontsize=16, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Autoencoder model and training functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Main Training Function with Full Visualization\n",
        "# =============================================================================\n",
        "\n",
        "def run_simple_training_fixed():\n",
        "    # Load data\n",
        "    data_matrix, protein_names, original_L = load_protein_data(\"Proteins_layer47\", normalize=True)\n",
        "    \n",
        "    print(f\"Data loaded: {data_matrix.shape}\")\n",
        "    input_dim = data_matrix.shape[1]  # Get actual input dimension\n",
        "    print(f\"Using input_dim = {input_dim}\")\n",
        "    \n",
        "    # Hyperparameters (exact as specified)\n",
        "    learning_rates = [0.01, 0.001, 0.0001]\n",
        "    weight_decays = [1e-4, 1e-5]\n",
        "    \n",
        "    best_score = float('inf')\n",
        "    best_params = None\n",
        "    \n",
        "    # Storage for visualization\n",
        "    all_results = []\n",
        "    all_fold_histories = []\n",
        "    all_hyperparams = []\n",
        "    \n",
        "    print(\"\\nStarting K-fold cross validation with hyperparameter grid search...\")\n",
        "    print(f\"Architecture: input_dim={input_dim}, proj_dim=1024, hidden_dim=128, latent_dim=8\")\n",
        "    print(f\"Training: 50 epochs, SGD, MSE loss, batch_size=64\")\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Grid search with K-fold CV\n",
        "    for lr in learning_rates:\n",
        "        for wd in weight_decays:\n",
        "            print(f\"\\nTesting lr={lr}, wd={wd}\")\n",
        "            \n",
        "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            scores = []\n",
        "            fold_histories = []\n",
        "            \n",
        "            for fold, (train_idx, val_idx) in enumerate(kfold.split(data_matrix)):\n",
        "                print(f\"  Fold {fold+1}/5\")\n",
        "                \n",
        "                # Split data\n",
        "                train_data = data_matrix[train_idx]\n",
        "                val_data = data_matrix[val_idx]\n",
        "                \n",
        "                # Create loaders (batch_size=64 as specified)\n",
        "                train_loader = DataLoader(TensorDataset(torch.FloatTensor(train_data)), \n",
        "                                        batch_size=64, shuffle=True)\n",
        "                val_loader = DataLoader(TensorDataset(torch.FloatTensor(val_data)), \n",
        "                                      batch_size=64, shuffle=False)\n",
        "                \n",
        "                # Train model with CORRECT input dimension and collect history\n",
        "                model = SimpleAutoencoder(input_dim=input_dim)\n",
        "                score, train_losses, val_losses = train_simple(\n",
        "                    model, train_loader, val_loader, lr, wd, device, return_history=True\n",
        "                )\n",
        "                scores.append(score)\n",
        "                fold_histories.append((train_losses, val_losses))\n",
        "            \n",
        "            mean_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"  Mean: {mean_score:.6f} ± {std_score:.6f}\")\n",
        "            \n",
        "            # Store results\n",
        "            all_results.append({\n",
        "                'lr': lr,\n",
        "                'wd': wd,\n",
        "                'mean_score': mean_score,\n",
        "                'std_score': std_score,\n",
        "                'scores': scores\n",
        "            })\n",
        "            all_fold_histories.append(fold_histories)\n",
        "            all_hyperparams.append({'lr': lr, 'wd': wd})\n",
        "            \n",
        "            if mean_score < best_score:\n",
        "                best_score = mean_score\n",
        "                best_params = {'lr': lr, 'wd': wd}\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"BEST RESULTS:\")\n",
        "    print(f\"Best params: lr={best_params['lr']}, wd={best_params['wd']}\")\n",
        "    print(f\"Best CV score: {best_score:.6f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    \n",
        "    # Plot training curves for all hyperparameter combinations\n",
        "    print(\"\\nGenerating training curve visualizations...\")\n",
        "    plot_training_curves(all_fold_histories, all_hyperparams)\n",
        "    \n",
        "    # Plot hyperparameter comparison\n",
        "    print(\"\\nGenerating hyperparameter comparison plots...\")\n",
        "    plot_hyperparameter_comparison(results_df)\n",
        "    \n",
        "    # Train final model with best parameters\n",
        "    print(\"\\nTraining final model with best parameters...\")\n",
        "    split_idx = int(0.8 * len(data_matrix))\n",
        "    train_data = data_matrix[:split_idx]\n",
        "    val_data = data_matrix[split_idx:]\n",
        "    \n",
        "    train_loader = DataLoader(TensorDataset(torch.FloatTensor(train_data)), \n",
        "                            batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(torch.FloatTensor(val_data)), \n",
        "                          batch_size=64, shuffle=False)\n",
        "    \n",
        "    # Final model with CORRECT input dimension\n",
        "    final_model = SimpleAutoencoder(input_dim=input_dim)\n",
        "    final_score, final_train_losses, final_val_losses = train_simple(\n",
        "        final_model, train_loader, val_loader, \n",
        "        best_params['lr'], best_params['wd'], device, return_history=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Final model validation score: {final_score:.6f}\")\n",
        "    \n",
        "    # Plot final training curve\n",
        "    print(\"\\nGenerating final model training curve...\")\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    epochs = range(1, len(final_train_losses) + 1)\n",
        "    ax.plot(epochs, final_train_losses, '--', label='Training Loss', linewidth=2)\n",
        "    ax.plot(epochs, final_val_losses, '-', label='Validation Loss', linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title(f'Final Model Training (lr={best_params[\"lr\"]}, wd={best_params[\"wd\"]:.0e})')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Save model\n",
        "    torch.save(final_model.state_dict(), 'simple_autoencoder_fixed.pth')\n",
        "    print(\"Model saved as 'simple_autoencoder_fixed.pth'\")\n",
        "    \n",
        "    return final_model, best_params, best_score, results_df\n",
        "\n",
        "print(\"Main training function with full visualization ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Execute Training with Visualizations\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING COMPREHENSIVE AUTOENCODER TRAINING WITH VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# First, visualize the model architecture\n",
        "print(\"\\nVisualizing Model Architecture...\")\n",
        "visualize_model_architecture(input_dim=2500)\n",
        "\n",
        "# Now run the actual training\n",
        "print(\"\\nStarting actual autoencoder training...\")\n",
        "final_model, best_params, best_score, results_df = run_simple_training_fixed()\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Best validation score: {best_score:.6f}\")\n",
        "\n",
        "# Display detailed results summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED TRAINING RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final MSE Loss: {best_score:.6f}\")\n",
        "print(f\"Best Learning Rate: {best_params['lr']}\")\n",
        "print(f\"Best Weight Decay: {best_params['wd']:.2e}\")\n",
        "print(\"\\nAll Hyperparameter Results:\")\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"  lr={row['lr']:.0e}, wd={row['wd']:.0e} → MSE: {row['mean_score']:.6f} ± {row['std_score']:.6f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Detailed Loss Analysis and Model Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_reconstruction_quality(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate reconstruction quality with detailed metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_mse = 0\n",
        "    total_mae = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    all_originals = []\n",
        "    all_reconstructions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            data = batch[0].to(device)\n",
        "            reconstructed = model(data)\n",
        "            \n",
        "            # Calculate MSE and MAE\n",
        "            mse = nn.MSELoss()(reconstructed, data)\n",
        "            mae = nn.L1Loss()(reconstructed, data)\n",
        "            \n",
        "            total_mse += mse.item() * data.size(0)\n",
        "            total_mae += mae.item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "            \n",
        "            # Store for visualization (sample a few)\n",
        "            if len(all_originals) < 100:  # Limit for memory\n",
        "                all_originals.extend(data.cpu().numpy())\n",
        "                all_reconstructions.extend(reconstructed.cpu().numpy())\n",
        "    \n",
        "    avg_mse = total_mse / total_samples\n",
        "    avg_mae = total_mae / total_samples\n",
        "    \n",
        "    return avg_mse, avg_mae, np.array(all_originals), np.array(all_reconstructions)\n",
        "\n",
        "def plot_reconstruction_analysis(originals, reconstructions):\n",
        "    \"\"\"\n",
        "    Plot detailed reconstruction analysis\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    # 1. Original vs Reconstructed scatter plot\n",
        "    sample_indices = np.random.choice(len(originals), min(1000, len(originals)), replace=False)\n",
        "    orig_sample = originals[sample_indices].flatten()\n",
        "    recon_sample = reconstructions[sample_indices].flatten()\n",
        "    \n",
        "    axes[0,0].scatter(orig_sample, recon_sample, alpha=0.5, s=1)\n",
        "    axes[0,0].plot([orig_sample.min(), orig_sample.max()], \n",
        "                   [orig_sample.min(), orig_sample.max()], 'r--', lw=2)\n",
        "    axes[0,0].set_xlabel('Original Values')\n",
        "    axes[0,0].set_ylabel('Reconstructed Values')\n",
        "    axes[0,0].set_title('Original vs Reconstructed Values')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Reconstruction error distribution\n",
        "    errors = (originals - reconstructions).flatten()\n",
        "    axes[0,1].hist(errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
        "    axes[0,1].set_xlabel('Reconstruction Error')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].set_title('Distribution of Reconstruction Errors')\n",
        "    axes[0,1].axvline(0, color='red', linestyle='--', label='Perfect Reconstruction')\n",
        "    axes[0,1].legend()\n",
        "    \n",
        "    # 3. Sample-wise MSE\n",
        "    sample_mse = np.mean((originals - reconstructions)**2, axis=1)\n",
        "    axes[0,2].hist(sample_mse, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[0,2].set_xlabel('Sample MSE')\n",
        "    axes[0,2].set_ylabel('Frequency')\n",
        "    axes[0,2].set_title('Distribution of Sample-wise MSE')\n",
        "    \n",
        "    # 4. Feature-wise reconstruction quality\n",
        "    feature_mse = np.mean((originals - reconstructions)**2, axis=0)\n",
        "    axes[1,0].plot(feature_mse[:1000])  # Plot first 1000 features\n",
        "    axes[1,0].set_xlabel('Feature Index')\n",
        "    axes[1,0].set_ylabel('Feature MSE')\n",
        "    axes[1,0].set_title('Feature-wise Reconstruction Error (First 1000)')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Correlation between original and reconstructed\n",
        "    correlations = []\n",
        "    for i in range(min(100, len(originals))):  # Sample 100 examples\n",
        "        corr = np.corrcoef(originals[i], reconstructions[i])[0,1]\n",
        "        if not np.isnan(corr):\n",
        "            correlations.append(corr)\n",
        "    \n",
        "    axes[1,1].hist(correlations, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1,1].set_xlabel('Correlation Coefficient')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].set_title('Sample-wise Correlation Distribution')\n",
        "    axes[1,1].axvline(np.mean(correlations), color='red', linestyle='--', \n",
        "                      label=f'Mean: {np.mean(correlations):.3f}')\n",
        "    axes[1,1].legend()\n",
        "    \n",
        "    # 6. Reconstruction quality by magnitude\n",
        "    orig_magnitudes = np.linalg.norm(originals, axis=1)\n",
        "    sample_errors = np.mean(np.abs(originals - reconstructions), axis=1)\n",
        "    \n",
        "    axes[1,2].scatter(orig_magnitudes, sample_errors, alpha=0.6, s=10)\n",
        "    axes[1,2].set_xlabel('Original Sample Magnitude')\n",
        "    axes[1,2].set_ylabel('Mean Absolute Error')\n",
        "    axes[1,2].set_title('Reconstruction Error vs Sample Magnitude')\n",
        "    axes[1,2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nReconstruction Quality Summary:\")\n",
        "    print(f\"Mean Reconstruction Error: {np.mean(errors):.6f}\")\n",
        "    print(f\"Std Reconstruction Error: {np.std(errors):.6f}\")\n",
        "    print(f\"Mean Sample MSE: {np.mean(sample_mse):.6f}\")\n",
        "    print(f\"Mean Sample Correlation: {np.mean(correlations):.4f}\")\n",
        "    print(f\"Min/Max Correlations: {np.min(correlations):.4f} / {np.max(correlations):.4f}\")\n",
        "\n",
        "print(\"Detailed evaluation functions ready!\")\n",
        "\n",
        "# Run detailed evaluation on the trained model\n",
        "if 'final_model' in locals():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING DETAILED MSE LOSS AND RECONSTRUCTION ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load data for evaluation\n",
        "    data_matrix, protein_names, original_L = load_protein_data(\"Proteins_layer47\", normalize=True)\n",
        "    \n",
        "    # Create test data loader\n",
        "    test_loader = DataLoader(TensorDataset(torch.FloatTensor(data_matrix)), \n",
        "                           batch_size=64, shuffle=False)\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # Evaluate reconstruction quality\n",
        "    print(\"Evaluating reconstruction quality...\")\n",
        "    mse, mae, originals, reconstructions = evaluate_reconstruction_quality(\n",
        "        final_model, test_loader, device)\n",
        "    \n",
        "    print(f\"\\nFinal Model Performance:\")\n",
        "    print(f\"MSE Loss: {mse:.6f}\")\n",
        "    print(f\"MAE Loss: {mae:.6f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mse):.6f}\")\n",
        "    \n",
        "    # Plot detailed reconstruction analysis\n",
        "    print(\"\\nGenerating detailed reconstruction analysis plots...\")\n",
        "    plot_reconstruction_analysis(originals, reconstructions)\n",
        "    \n",
        "else:\n",
        "    print(\"No trained model found. Please run the training first!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM5U2XhMCFnCKIWuxZLNUJn",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
